//
//  Renderer.swift
//  SceneDepthPointCloud

import Metal
import MetalKit
import ARKit
import CoreImage
import Foundation
import Photos


// MARK: - OpenSplat Native Format
/// OpenSplat-native Gaussian structure (stores scale+quaternion directly)
/// Matches ShaderTypes.h::OpenSplatGaussian layout
struct OpenSplatGaussian {
    var position_x: Float       // offset 0
    var position_y: Float       // offset 4
    var position_z: Float       // offset 8
    var scale_x: Float          // offset 12
    var scale_y: Float          // offset 16
    var scale_z: Float          // offset 20
    var quat_x: Float           // offset 24
    var quat_y: Float           // offset 28
    var quat_z: Float           // offset 32
    var quat_w: Float           // offset 36
    var color_r: Float16        // offset 40
    var color_g: Float16        // offset 42
    var color_b: Float16        // offset 44
    var opacity: Float16        // offset 46
    // Swift will pad this struct - Metal struct must match the padded size!

    static func verifySizeMatches() {
        let size = MemoryLayout<OpenSplatGaussian>.size
        let stride = MemoryLayout<OpenSplatGaussian>.stride
        print("üìè OpenSplatGaussian Swift layout: size=\(size), stride=\(stride)")
        print("   Metal struct must be padded to \(stride) bytes to match!")
    }
}

// MARK: - Render Mode
enum RenderMode {
    case particles  // Traditional point cloud rendering
    case splats     // Gaussian splat rendering
    case both       // Render both (particles then splats)
}

// MARK: - Core Metal Scan Renderer
final class Renderer {
    var savedCloudURLs = [URL]()
    private var cpuParticlesBuffer = [CPUParticle]()
    var showParticles = true
    var isInViewSceneMode = true
    var isSavingFile = false
    var highConfCount = 0
    var savingError: XError? = nil

    // MARK: - Gaussian Splat Rendering
    var openSplatRenderer: OpenSplatRenderer?

    var renderMode: RenderMode = .splats {
        didSet {
            print("üé® Render mode changed: \(renderMode)")
        }
    }
    // Maximum number of points we store in the point cloud
    private let maxPoints = 15_000_000
    // Number of sample points on the grid
    var numGridPoints = 2_200  // Increased by 10% from 2000
    // Particle's size in pixels
    private let particleSize: Float = 8

    // LiDAR/Point cloud processing frame rate control
    var lidarProcessingFPS: Double = 3.0 {
        didSet {
            lidarProcessingInterval = 1.0 / lidarProcessingFPS
        }
    }
    private var lidarProcessingInterval: TimeInterval = 1.0 / 3.0
    private var lastLidarProcessingTime: TimeInterval = 0

    // Data extraction system for Gaussian Splatting
    var isDataExtractionEnabled = false
    var dataExtractionFPS: Double = 3.0 {
        didSet {
            dataExtractionInterval = 1.0 / dataExtractionFPS
        }
    }
    private var dataExtractionInterval: TimeInterval = 1.0 / 3.0
    private var lastDataExtractionTime: TimeInterval = 0
    private var frameCounter = 0
    private var extractedFrameCounter = 0
    private var currentScanFolderName = ""

    // Debug logging control
    private var debugFrameCounter = 0
    private let debugLogInterval = 60  // Log every N frames

    // Frame timing for OpenSplat performance monitoring
    private var renderFrameCounter = 0
    private var lastFrameTimestamp: TimeInterval = 0
    private let frameTimingInterval = 10  // Report timing every N frames
    // We only use portrait orientation in this app
    private let orientation = UIInterfaceOrientation.portrait
    // Camera's threshold values for detecting when the camera moves so that we can accumulate the points
    // set to 0 for continous sampling
    private let cameraRotationThreshold = cos(0 * .degreesToRadian)
    private let cameraTranslationThreshold: Float = pow(0.00, 2)   // (meter-squared)
    // The max number of command buffers in flight
    private let maxInFlightBuffers = 5

    private lazy var rotateToARCamera = Self.makeRotateToARCameraMatrix(orientation: orientation)
    private let session: ARSession

    // Metal objects and textures
    private let device: MTLDevice
    private let library: MTLLibrary
    private let renderDestination: RenderDestinationProvider
    private let relaxedStencilState: MTLDepthStencilState
    private let depthStencilState: MTLDepthStencilState
    private var commandQueue: MTLCommandQueue
    private lazy var unprojectPipelineState = makeUnprojectionPipelineState()!
    private lazy var unprojectOpenSplatPipelineState = makeUnprojectionOpenSplatPipelineState()!
    private lazy var unpackGaussiansPipelineState: MTLComputePipelineState? = makeUnpackGaussiansPipelineState()
    private lazy var rgbPipelineState = makeRGBPipelineState()!
    private lazy var particlePipelineState = makeParticlePipelineState()!
    // texture cache for captured image
    private lazy var textureCache = makeTextureCache()
    private var capturedImageTextureY: CVMetalTexture?
    private var capturedImageTextureCbCr: CVMetalTexture?
    private var depthTexture: CVMetalTexture?
    private var confidenceTexture: CVMetalTexture?

    // Multi-buffer rendering pipeline
    private let inFlightSemaphore: DispatchSemaphore
    private var currentBufferIndex = 0

    // The current viewport size
    private var viewportSize = CGSize()
    // The grid of sample points
    private lazy var gridPointsBuffer = MetalBuffer<Float2>(device: device,
                                                            array: makeGridPoints(),
                                                            index: kGridPoints.rawValue, options: [])

    // RGB buffer
    private lazy var rgbUniforms: RGBUniforms = {
        var uniforms = RGBUniforms()
        uniforms.radius = rgbOn ? 2 : 0
        uniforms.viewToCamera.copy(from: viewToCamera)
        uniforms.viewRatio = Float(viewportSize.width / viewportSize.height)
        return uniforms
    }()
    private var rgbUniformsBuffers = [MetalBuffer<RGBUniforms>]()
    // Point Cloud buffer
    private lazy var pointCloudUniforms: PointCloudUniforms = {
        var uniforms = PointCloudUniforms()
        uniforms.maxPoints = Int32(maxPoints)
        uniforms.confidenceThreshold = Int32(confidenceThreshold)
        uniforms.particleSize = particleSize
        uniforms.cameraResolution = cameraResolution
        return uniforms
    }()
    private var pointCloudUniformsBuffers = [MetalBuffer<PointCloudUniforms>]()
    // Particles buffer
    private var particlesBuffer: MetalBuffer<ParticleUniforms>
    private var currentPointIndex = 0
    private var currentPointCount = 0

    // OpenSplat-native buffer (stores scale+quaternion directly, no covariance)
    private var openSplatGaussiansBuffer: SplatMetalBuffer<OpenSplatGaussian>!
    private var currentOpenSplatIndex = 0
    private var currentOpenSplatCount = 0
    
    // Cached unpacked buffers for OpenSplat (allocated once, reused every frame)
    private var cachedMeans3dBuffer: MTLBuffer?
    private var cachedScalesBuffer: MTLBuffer?
    private var cachedQuatsBuffer: MTLBuffer?
    private var cachedColorsBuffer: MTLBuffer?
    private var cachedOpacitiesBuffer: MTLBuffer?
    private var cachedBufferCapacity: Int = 0
    private var cachedGaussianData: OpenSplatRenderer.GaussianData?
    private var lastUnpackedCount: Int = 0

    // Camera data
    private var sampleFrame: ARFrame { session.currentFrame! }
    private lazy var cameraResolution = Float2(Float(sampleFrame.camera.imageResolution.width), Float(sampleFrame.camera.imageResolution.height))
    private lazy var viewToCamera = sampleFrame.displayTransform(for: orientation, viewportSize: viewportSize).inverted()
    private lazy var lastCameraTransform = sampleFrame.camera.transform

    // interfaces
    var confidenceThreshold = 2
    
    // Depth source selection
    var depthSource: DepthSource = .lidar
    
    // PromptDA + LoG sampling support
    private var promptDAEngine: PromptDAEngine?
    private var cachedProbSamples: [Float2]?
    
    // Depth photo saving
    private var depthPhotoCounter = 0
    private var shouldSaveNextDepth = false

    var rgbOn: Bool = false {
        didSet {
            // apply the change for the shader
            rgbUniforms.radius = rgbOn ? 2 : 0
        }
    }

    init(session: ARSession, metalDevice device: MTLDevice, renderDestination: RenderDestinationProvider) {
        self.session = session
        self.device = device
        self.renderDestination = renderDestination
        library = device.makeDefaultLibrary()!

        commandQueue = device.makeCommandQueue()!
        // initialize our buffers
        for _ in 0 ..< maxInFlightBuffers {
            rgbUniformsBuffers.append(.init(device: device, count: 1, index: 0))
            pointCloudUniformsBuffers.append(.init(device: device, count: 1, index: kPointCloudUniforms.rawValue))
        }
        particlesBuffer = .init(device: device, count: maxPoints, index: kParticleUniforms.rawValue)

        // Verify struct layout and print actual Swift stride
        OpenSplatGaussian.verifySizeMatches()

        // Use Swift's natural stride - Metal struct will be padded to match
        openSplatGaussiansBuffer = try! SplatMetalBuffer(device: device, capacity: maxPoints)
        // rbg does not need to read/write depth
        let relaxedStateDescriptor = MTLDepthStencilDescriptor()
        relaxedStencilState = device.makeDepthStencilState(descriptor: relaxedStateDescriptor)!

        // setup depth test for point cloud
        let depthStateDescriptor = MTLDepthStencilDescriptor()
        depthStateDescriptor.depthCompareFunction = .lessEqual
        depthStateDescriptor.isDepthWriteEnabled = true
        depthStencilState = device.makeDepthStencilState(descriptor: depthStateDescriptor)!

        inFlightSemaphore = DispatchSemaphore(value: maxInFlightBuffers)
        
        // Try to initialize PromptDA engine (will be used if depthSource == .mvs)
        print("\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
        print("üîß Renderer: Initializing PromptDA Engine...")
        print("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
        do {
            let startTime = CFAbsoluteTimeGetCurrent()
            // Use 256√ó192 prompt size (matches ARKit depth directly, no rotation)
            promptDAEngine = try PromptDAEngine.create(
                bundleModelName: "PromptDA_vits_518x518_prompt256x192",
                rgbSize: .init(width: 518, height: 518),
                promptHW: (256, 192)
            )
            let elapsed = CFAbsoluteTimeGetCurrent() - startTime
            print("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
            print("‚úÖ Renderer: PromptDA ready! (loaded in \(String(format: "%.2f", elapsed))s)")
            print("   MVS mode is available")
            print("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n")
        } catch {
            print("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
            print("‚ö†Ô∏è Renderer: PromptDA NOT available")
            print("   Error: \(error.localizedDescription)")
            if let nsError = error as NSError? {
                print("   Domain: \(nsError.domain)")
                print("   Code: \(nsError.code)")
                if let reason = nsError.userInfo[NSLocalizedFailureReasonErrorKey] as? String {
                    print("   Reason: \(reason)")
                }
            }
            print("   ‚Üí MVS mode will fall back to LiDAR")
            print("   ‚Üí Add the PromptDA .mlpackage to Xcode project to enable MVS")
            print("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n")
        }

        // Initialize OpenSplatRenderer for tile-based rendering
        print("\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
        print("üîß Renderer: Initializing OpenSplatRenderer...")
        print("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
        do {
            let startTime = CFAbsoluteTimeGetCurrent()
            openSplatRenderer = try OpenSplatRenderer(device: device)

            let elapsed = CFAbsoluteTimeGetCurrent() - startTime
            print("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
            print("‚úÖ Renderer: OpenSplatRenderer ready! (loaded in \(String(format: "%.2f", elapsed))s)")
            print("   Tile-based GPU rendering with GPU zero-copy unpacking")
            print("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n")
        } catch {
            print("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
            print("‚ö†Ô∏è Renderer: OpenSplatRenderer NOT available")
            print("   Error: \(error.localizedDescription)")
            if let nsError = error as NSError? {
                print("   Domain: \(nsError.domain)")
                print("   Code: \(nsError.code)")
            }
            print("   ‚Üí Gaussian rendering will not be available")
            print("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n")
        }

        self.loadSavedClouds()
    }

    func drawRectResized(size: CGSize) {
        viewportSize = size
    }

    private func updateCapturedImageTextures(frame: ARFrame) {
        // Create two textures (Y and CbCr) from the provided frame's captured image
        let pixelBuffer = frame.capturedImage
        guard CVPixelBufferGetPlaneCount(pixelBuffer) >= 2 else {
            return
        }

        capturedImageTextureY = makeTexture(fromPixelBuffer: pixelBuffer, pixelFormat: .r8Unorm, planeIndex: 0)
        capturedImageTextureCbCr = makeTexture(fromPixelBuffer: pixelBuffer, pixelFormat: .rg8Unorm, planeIndex: 1)
    }

    private func updateDepthTextures(frame: ARFrame) -> Bool {
        // Use different depth processing based on depth source
        switch depthSource {
        case .mvs:
            return updateMVSDepthTextures(frame: frame)
        case .lidar, .depthView:
            return updateLiDARDepthTextures(frame: frame)
        }
    }
    
    private func updateLiDARDepthTextures(frame: ARFrame) -> Bool {
        guard let depthMap = frame.smoothedSceneDepth?.depthMap,
            let confidenceMap = frame.smoothedSceneDepth?.confidenceMap else {
                print("‚ö†Ô∏è LiDAR: No smoothed scene depth available")
                return false
        }

        let depthW = CVPixelBufferGetWidth(depthMap)
        let depthH = CVPixelBufferGetHeight(depthMap)
        let confW = CVPixelBufferGetWidth(confidenceMap)
        let confH = CVPixelBufferGetHeight(confidenceMap)
        
        print("üìç LiDAR Mode:")
        print("   ‚Ä¢ Smoothed depth resolution: \(depthW)√ó\(depthH)")
        print("   ‚Ä¢ Confidence map resolution: \(confW)√ó\(confH)")
        print("   ‚Ä¢ Camera resolution: \(Int(cameraResolution.x))√ó\(Int(cameraResolution.y))")
        print("   ‚Ä¢ Grid points: \(numGridPoints) (uniform grid)")

        depthTexture = makeTexture(fromPixelBuffer: depthMap, pixelFormat: .r32Float, planeIndex: 0)
        confidenceTexture = makeTexture(fromPixelBuffer: confidenceMap, pixelFormat: .r8Uint, planeIndex: 0)
        
        // Save depth photo if requested
        if shouldSaveNextDepth {
            saveDepthAsPhoto(depthMap)
            shouldSaveNextDepth = false
        }
        
        // Clear cached probability samples (use uniform grid for LiDAR)
        cachedProbSamples = nil

        return true
    }
    
    // Save ARKit depth map as a photo in Photos app
    private func saveDepthAsPhoto(_ depthPixelBuffer: CVPixelBuffer) {
        depthPhotoCounter += 1
        
        print("üì∏ Saving ARKit depth map as photo...")
        
        // Convert depth to visible image (normalize to 0-255)
        let width = CVPixelBufferGetWidth(depthPixelBuffer)
        let height = CVPixelBufferGetHeight(depthPixelBuffer)
        
        CVPixelBufferLockBaseAddress(depthPixelBuffer, .readOnly)
        defer { CVPixelBufferUnlockBaseAddress(depthPixelBuffer, .readOnly) }
        
        let stride = CVPixelBufferGetBytesPerRow(depthPixelBuffer) / MemoryLayout<Float>.stride
        let depthData = CVPixelBufferGetBaseAddress(depthPixelBuffer)!.assumingMemoryBound(to: Float.self)
        
        // Find min/max for normalization
        var minDepth: Float = .infinity
        var maxDepth: Float = -.infinity
        for y in 0..<height {
            for x in 0..<width {
                let depth = depthData[y * stride + x]
                if depth > 0 && depth.isFinite {
                    minDepth = min(minDepth, depth)
                    maxDepth = max(maxDepth, depth)
                }
            }
        }
        
        print("   ‚Ä¢ Depth range: \(String(format: "%.3f", minDepth))m - \(String(format: "%.3f", maxDepth))m")
        
        // Create grayscale image
        var grayData = [UInt8](repeating: 0, count: width * height)
        let range = max(maxDepth - minDepth, 0.001)
        
        for y in 0..<height {
            for x in 0..<width {
                let depth = depthData[y * stride + x]
                let normalized = depth > 0 && depth.isFinite ? (depth - minDepth) / range : 0
                grayData[y * width + x] = UInt8(max(0, min(255, normalized * 255)))
            }
        }
        
        // Create CGImage from gray data
        let colorSpace = CGColorSpaceCreateDeviceGray()
        let bitmapInfo = CGBitmapInfo(rawValue: CGImageAlphaInfo.none.rawValue)
        
        guard let providerRef = CGDataProvider(data: Data(grayData) as CFData),
              let cgImage = CGImage(width: width,
                                   height: height,
                                   bitsPerComponent: 8,
                                   bitsPerPixel: 8,
                                   bytesPerRow: width,
                                   space: colorSpace,
                                   bitmapInfo: bitmapInfo,
                                   provider: providerRef,
                                   decode: nil,
                                   shouldInterpolate: false,
                                   intent: .defaultIntent) else {
            print("   ‚ùå Failed to create CGImage from depth data")
            return
        }
        
        let uiImage = UIImage(cgImage: cgImage)
        
        // Save to Photos
        PHPhotoLibrary.requestAuthorization { status in
            guard status == .authorized else {
                print("   ‚ùå Photos permission not granted")
                return
            }
            
            PHPhotoLibrary.shared().performChanges({
                PHAssetChangeRequest.creationRequestForAsset(from: uiImage)
            }) { success, error in
                if success {
                    print("   ‚úÖ Depth photo #\(self.depthPhotoCounter) saved to Photos (\(width)√ó\(height))")
                } else {
                    print("   ‚ùå Failed to save depth photo: \(error?.localizedDescription ?? "unknown error")")
                }
            }
        }
    }
    
    private func updateMVSDepthTextures(frame: ARFrame) -> Bool {
        guard let promptDA = promptDAEngine else {
            print("‚ö†Ô∏è PromptDA not available, falling back to LiDAR")
            return updateLiDARDepthTextures(frame: frame)
        }
        
        // Log input data
        let rgbW = CVPixelBufferGetWidth(frame.capturedImage)
        let rgbH = CVPixelBufferGetHeight(frame.capturedImage)
        print("\nüìç MVS Mode (PromptDA):")
        print("   ‚Ä¢ Input RGB resolution: \(rgbW)√ó\(rgbH)")
        
        if let lidarPrompt = frame.smoothedSceneDepth?.depthMap {
            let promptW = CVPixelBufferGetWidth(lidarPrompt)
            let promptH = CVPixelBufferGetHeight(lidarPrompt)
            print("   ‚Ä¢ Input LiDAR prompt resolution: \(promptW)√ó\(promptH)")
        } else {
            print("   ‚Ä¢ No LiDAR prompt available")
        }
        
        do {
            // Run PromptDA with LiDAR as prompt (if available)
            let lidarPrompt = frame.smoothedSceneDepth?.depthMap
            let output = try promptDA.predict(rgbPB: frame.capturedImage, lidarPB: lidarPrompt)
            
            // Use PromptDA depth instead of LiDAR
            guard let newDepthTexture = makeTexture(fromPixelBuffer: output.depthPB, pixelFormat: .r32Float, planeIndex: 0) else {
                print("‚ùå Failed to create Metal texture from PromptDA depth output")
                return updateLiDARDepthTextures(frame: frame)
            }
            depthTexture = newDepthTexture
            
            // Generate LoG-based probability samples
            let depthW = CVPixelBufferGetWidth(output.depthPB)
            let depthH = CVPixelBufferGetHeight(output.depthPB)
            let probW = CVPixelBufferGetWidth(output.probPB)
            let probH = CVPixelBufferGetHeight(output.probPB)
            
            print("   ‚Ä¢ PromptDA output depth: \(depthW)√ó\(depthH)")
            print("   ‚Ä¢ LoG probability map: \(probW)√ó\(probH)")
            
            // Use Bernoulli sampling for intelligent point placement (stochastic sample count)
            let pixels = ProbabilitySampler.samplePixelsBernoulli(probPB: output.probPB, scale: 2)
            
            print("   ‚Ä¢ Sampled \(pixels.count) points from LoG probability distribution (Bernoulli)")
            
            // Convert probability samples from PromptDA resolution to camera resolution
            // PromptDA outputs at 518√ó518, but camera is 1920√ó1440
            let scaleX = Float(cameraResolution.x) / Float(probW)
            let scaleY = Float(cameraResolution.y) / Float(probH)
            print("   ‚Ä¢ Scaling samples to camera resolution: scaleX=\(String(format: "%.2f", scaleX)), scaleY=\(String(format: "%.2f", scaleY))")
            
            cachedProbSamples = pixels.map { pixel in
                Float2(Float(pixel.x) * scaleX, Float(pixel.y) * scaleY)
            }
            
            print("   ‚Ä¢ Final sample count for rendering: \(cachedProbSamples?.count ?? 0)")
            
            // Use high confidence for PromptDA points (or reuse LiDAR confidence if available)
            if let confMap = frame.smoothedSceneDepth?.confidenceMap {
                confidenceTexture = makeTexture(fromPixelBuffer: confMap, pixelFormat: .r8Uint, planeIndex: 0)
                print("   ‚Ä¢ Using LiDAR confidence map")
            } else {
                // Generate synthetic high confidence map
                confidenceTexture = makeSyntheticConfidenceTexture(width: depthW, height: depthH)
                print("   ‚Ä¢ Using synthetic confidence map (all high confidence)")
            }
            
            // Ensure confidence texture was created
            if confidenceTexture == nil {
                print("‚ùå Failed to create confidence texture")
                return updateLiDARDepthTextures(frame: frame)
            }
            
            print("‚úÖ PromptDA MVS complete\n")
            return true
            
        } catch {
            print("‚ùå PromptDA failed: \(error.localizedDescription), falling back to LiDAR")
            cachedProbSamples = nil
            return updateLiDARDepthTextures(frame: frame)
        }
    }
    
    private func makeSyntheticConfidenceTexture(width: Int, height: Int) -> CVMetalTexture? {
        var pb: CVPixelBuffer?
        let status = CVPixelBufferCreate(kCFAllocatorDefault, width, height, 
                                        kCVPixelFormatType_OneComponent8, nil, &pb)
        guard status == kCVReturnSuccess, let pixelBuffer = pb else { return nil }
        
        CVPixelBufferLockBaseAddress(pixelBuffer, [])
        defer { CVPixelBufferUnlockBaseAddress(pixelBuffer, []) }
        
        let buf = CVPixelBufferGetBaseAddress(pixelBuffer)!.assumingMemoryBound(to: UInt8.self)
        let stride = CVPixelBufferGetBytesPerRow(pixelBuffer)
        
        // Fill with high confidence (2)
        for y in 0..<height {
            for x in 0..<width {
                buf[y * stride + x] = 2
            }
        }
        
        return makeTexture(fromPixelBuffer: pixelBuffer, pixelFormat: .r8Uint, planeIndex: 0)
    }

    private func update(frame: ARFrame) {
        // frame dependent info
        let camera = frame.camera
        let cameraIntrinsicsInversed = camera.intrinsics.inverse
        let viewMatrix = camera.viewMatrix(for: orientation)
        let viewMatrixInversed = viewMatrix.inverse
        let projectionMatrix = camera.projectionMatrix(for: orientation, viewportSize: viewportSize, zNear: 0.001, zFar: 0)
        pointCloudUniforms.viewProjectionMatrix = projectionMatrix * viewMatrix
        pointCloudUniforms.localToWorld = viewMatrixInversed * rotateToARCamera
        pointCloudUniforms.cameraIntrinsicsInversed = cameraIntrinsicsInversed
    }

        func draw() {
        debugFrameCounter += 1
        let shouldLog = debugFrameCounter % debugLogInterval == 0

        guard let currentFrame = session.currentFrame else { return }
        guard let renderDescriptor = renderDestination.currentRenderPassDescriptor else { return }
        guard let commandBuffer = commandQueue.makeCommandBuffer() else { return }
        guard let renderEncoder = commandBuffer.makeRenderCommandEncoder(descriptor: renderDescriptor) else { return }

        _ = inFlightSemaphore.wait(timeout: DispatchTime.distantFuture)
        commandBuffer.addCompletedHandler { [weak self] commandBuffer in
            if let self = self {
                self.inFlightSemaphore.signal()
            }
        }

        // Always update frame data and camera textures for smooth camera feed
        update(frame: currentFrame)
        updateCapturedImageTextures(frame: currentFrame)

        // handle buffer rotating
        currentBufferIndex = (currentBufferIndex + 1) % maxInFlightBuffers
        pointCloudUniformsBuffers[currentBufferIndex][0] = pointCloudUniforms

        // Extract data for Gaussian Splatting if enabled
        if shouldExtractDataThisFrame(currentFrame) {
            extractCameraData(frame: currentFrame)
        }

        // Only process LiDAR/point cloud data at the throttled rate
        let willProcessLiDAR = shouldProcessLiDARThisFrame(currentFrame)
        let willAccumulate = shouldAccumulate(frame: currentFrame)
        print("üîç LIDAR: willProcessLiDAR=\(willProcessLiDAR), willAccumulate=\(willAccumulate)")

        if willProcessLiDAR && willAccumulate, updateDepthTextures(frame: currentFrame) {
            print("‚úÖ ACCUMULATING points this frame...")
            accumulatePoints(frame: currentFrame, commandBuffer: commandBuffer, renderEncoder: renderEncoder)
        } else {
            print("‚è≠Ô∏è  SKIPPING accumulation (willProcess=\(willProcessLiDAR), willAccum=\(willAccumulate), currentOpenSplatCount=\(currentOpenSplatCount))")
        }

        // check and render rgb camera image
        if rgbUniforms.radius > 0 {
            var retainingTextures = [capturedImageTextureY, capturedImageTextureCbCr]
            commandBuffer.addCompletedHandler { buffer in
                retainingTextures.removeAll()
            }
            rgbUniformsBuffers[currentBufferIndex][0] = rgbUniforms

            renderEncoder.setDepthStencilState(relaxedStencilState)
            renderEncoder.setRenderPipelineState(rgbPipelineState)
            renderEncoder.setVertexBuffer(rgbUniformsBuffers[currentBufferIndex])
            renderEncoder.setFragmentBuffer(rgbUniformsBuffers[currentBufferIndex])
            renderEncoder.setFragmentTexture(CVMetalTextureGetTexture(capturedImageTextureY!), index: Int(kTextureY.rawValue))
            renderEncoder.setFragmentTexture(CVMetalTextureGetTexture(capturedImageTextureCbCr!), index: Int(kTextureCbCr.rawValue))
            renderEncoder.drawPrimitives(type: .triangleStrip, vertexStart: 0, vertexCount: 4)
        }

        // render particles (if enabled by render mode)
        if self.showParticles && (renderMode == .particles || renderMode == .both) {
            renderEncoder.setDepthStencilState(depthStencilState)
            renderEncoder.setRenderPipelineState(particlePipelineState)
            renderEncoder.setVertexBuffer(pointCloudUniformsBuffers[currentBufferIndex])
            renderEncoder.setVertexBuffer(particlesBuffer)
            renderEncoder.drawPrimitives(type: .point, vertexStart: 0, vertexCount: currentPointCount)
        }

        // End particle rendering encoder before splat rendering
        renderEncoder.endEncoding()

        // Render Gaussian splats (if enabled by render mode)
        if (renderMode == .splats || renderMode == .both) {
            if let openSplatRenderer = openSplatRenderer {
                // IMPORTANT: Commit accumulation buffer BEFORE OpenSplat rendering
                // This ensures GPU writes from accumulation are visible
                commandBuffer.commit()
                commandBuffer.waitUntilCompleted()

                // Create new command buffer for OpenSplat rendering
                guard let openSplatCommandBuffer = commandQueue.makeCommandBuffer() else { return }

                // Use OpenSplat tile-based renderer with GPU zero-copy unpacking
                renderWithOpenSplat(
                    commandBuffer: openSplatCommandBuffer,
                    openSplatRenderer: openSplatRenderer,
                    frame: currentFrame
                )

                // Present and commit OpenSplat rendering
                openSplatCommandBuffer.present(renderDestination.currentDrawable!)
                openSplatCommandBuffer.commit()
                return  // Early return since we already presented
            } else {
                // No renderer available
                commandBuffer.present(renderDestination.currentDrawable!)
                commandBuffer.commit()
                return
            }
        }

        // Only reached if not rendering splats
        commandBuffer.present(renderDestination.currentDrawable!)
        commandBuffer.commit()

        // Log summary every N frames
        if shouldLog {
            let particleCount = renderMode == .splats ? 0 : currentPointCount
            let splatCount = (renderMode == .splats || renderMode == .both) ? currentOpenSplatCount : 0
            print("üìä [RENDER] Particles: \(particleCount), Splats: \(splatCount) (frame \(debugFrameCounter))")
        }
    }

    private func shouldProcessLiDARThisFrame(_ frame: ARFrame) -> Bool {
        let currentTime = frame.timestamp
        if currentTime - lastLidarProcessingTime >= lidarProcessingInterval {
            lastLidarProcessingTime = currentTime
            return true
        }
        return false
    }

    // Helper function to get current processing rate for UI feedback
    func getCurrentProcessingRate() -> String {
        return String(format: "%.1f FPS", lidarProcessingFPS)
    }

    private func shouldExtractDataThisFrame(_ frame: ARFrame) -> Bool {
        guard isDataExtractionEnabled else { return false }

        let currentTime = frame.timestamp
        if currentTime - lastDataExtractionTime >= dataExtractionInterval {
            lastDataExtractionTime = currentTime
            return true
        }
        return false
    }

    private func shouldAccumulate(frame: ARFrame) -> Bool {
        if self.isInViewSceneMode || !self.showParticles {
            return false
        }
        let cameraTransform = frame.camera.transform
        return currentPointCount == 0
            || dot(cameraTransform.columns.2, lastCameraTransform.columns.2) <= cameraRotationThreshold
            || distance_squared(cameraTransform.columns.3, lastCameraTransform.columns.3) >= cameraTranslationThreshold
    }

    private func accumulatePoints(frame: ARFrame, commandBuffer: MTLCommandBuffer, renderEncoder: MTLRenderCommandEncoder) {
        pointCloudUniforms.pointCloudCurrentIndex = Int32(currentPointIndex)

        var retainingTextures = [capturedImageTextureY, capturedImageTextureCbCr, depthTexture, confidenceTexture]

        commandBuffer.addCompletedHandler { buffer in
            retainingTextures.removeAll()
            // copy gpu point buffer to cpu
            var i = self.cpuParticlesBuffer.count
            while (i < self.maxPoints && self.particlesBuffer[i].position != simd_float3(0.0,0.0,0.0)) {
                let position = self.particlesBuffer[i].position
                let color = self.particlesBuffer[i].color
                let confidence = self.particlesBuffer[i].confidence
                if confidence == 2 { self.highConfCount += 1 }
                self.cpuParticlesBuffer.append(
                    CPUParticle(position: position,
                                color: color,
                                confidence: confidence))
                i += 1
            }
        }
        
        // Regenerate grid points buffer if we have new probability samples (MVS mode)
        if depthSource == .mvs && cachedProbSamples != nil {
            let newPoints = makeGridPoints()
            print("   ‚Ä¢ Regenerating grid points buffer: \(newPoints.count) LoG-sampled points")
            gridPointsBuffer = MetalBuffer<Float2>(device: device,
                                                  array: newPoints,
                                                  index: kGridPoints.rawValue, options: [])
        }

        // Draw particles to particlesBuffer
        renderEncoder.setDepthStencilState(relaxedStencilState)
        renderEncoder.setRenderPipelineState(unprojectPipelineState)
        renderEncoder.setVertexBuffer(pointCloudUniformsBuffers[currentBufferIndex])
        renderEncoder.setVertexBuffer(particlesBuffer)
        renderEncoder.setVertexBuffer(gridPointsBuffer)
        renderEncoder.setVertexTexture(CVMetalTextureGetTexture(capturedImageTextureY!), index: Int(kTextureY.rawValue))
        renderEncoder.setVertexTexture(CVMetalTextureGetTexture(capturedImageTextureCbCr!), index: Int(kTextureCbCr.rawValue))
        renderEncoder.setVertexTexture(CVMetalTextureGetTexture(depthTexture!), index: Int(kTextureDepth.rawValue))
        renderEncoder.setVertexTexture(CVMetalTextureGetTexture(confidenceTexture!), index: Int(kTextureConfidence.rawValue))
        renderEncoder.drawPrimitives(type: .point, vertexStart: 0, vertexCount: gridPointsBuffer.count)

        print("   ‚Ä¢ Drawing \(gridPointsBuffer.count) grid points to accumulate particles")
        print("   ‚Ä¢ Particles: currentPointIndex=\(currentPointIndex), currentPointCount=\(currentPointCount)")

        // Draw to OpenSplat-native buffer (stores scale+quaternion directly, NO covariance!)
        // This eliminates per-frame CPU eigendecomposition
        print("   ‚Ä¢ Setting up OpenSplat pipeline...")
        print("   ‚Ä¢ OpenSplat: currentOpenSplatIndex=\(currentOpenSplatIndex), currentOpenSplatCount=\(currentOpenSplatCount)")
        renderEncoder.setRenderPipelineState(unprojectOpenSplatPipelineState)
        renderEncoder.setVertexBuffer(pointCloudUniformsBuffers[currentBufferIndex].buffer, offset: 0, index: 0)  // kPointCloudUniforms = 0
        renderEncoder.setVertexBuffer(gridPointsBuffer.buffer, offset: 0, index: 2)  // kGridPoints = 2
        renderEncoder.setVertexBuffer(openSplatGaussiansBuffer.buffer, offset: 0, index: 3)  // kGaussianUniforms = 3
        renderEncoder.setVertexTexture(CVMetalTextureGetTexture(capturedImageTextureY!), index: Int(kTextureY.rawValue))
        renderEncoder.setVertexTexture(CVMetalTextureGetTexture(capturedImageTextureCbCr!), index: Int(kTextureCbCr.rawValue))
        renderEncoder.setVertexTexture(CVMetalTextureGetTexture(depthTexture!), index: Int(kTextureDepth.rawValue))
        renderEncoder.setVertexTexture(CVMetalTextureGetTexture(confidenceTexture!), index: Int(kTextureConfidence.rawValue))
        print("   ‚Ä¢ Drawing \(gridPointsBuffer.count) grid points to accumulate OpenSplat gaussians...")
        renderEncoder.drawPrimitives(type: .point, vertexStart: 0, vertexCount: gridPointsBuffer.count)
        print("   ‚Ä¢ OpenSplat draw complete")

        currentPointIndex = (currentPointIndex + gridPointsBuffer.count) % maxPoints
        currentPointCount = min(currentPointCount + gridPointsBuffer.count, maxPoints)
        currentOpenSplatIndex = (currentOpenSplatIndex + gridPointsBuffer.count) % maxPoints
        currentOpenSplatCount = min(currentOpenSplatCount + gridPointsBuffer.count, maxPoints)

        // Update buffer count to match current count
        // The GPU shader writes to the buffer, but SplatMetalBuffer.count must be updated manually
        openSplatGaussiansBuffer.count = currentOpenSplatCount

        print("   ‚Ä¢ Current accumulated points: \(currentPointCount)/\(maxPoints)")
        print("   ‚Ä¢ Current accumulated OpenSplat gaussians: \(currentOpenSplatCount)/\(maxPoints)")
        print("   ‚Ä¢ OpenSplat buffer count updated: \(openSplatGaussiansBuffer.count)")

        // DEBUG: These reads happen BEFORE GPU executes the draw commands, so they'll show zeros
        // The actual data verification happens in renderWithOpenSplat after unpacking
        // print("   ‚Ä¢ Checking if OpenSplat buffer was written...")
        // let firstGaussian = openSplatGaussiansBuffer.values[0]
        // print("     First gaussian after draw: pos=(\(firstGaussian.position.x), \(firstGaussian.position.y), \(firstGaussian.position.z))")

        lastCameraTransform = frame.camera.transform
    }
}

// MARK:  - Added Renderer functionality
extension Renderer {
    func toggleParticles() {
        self.showParticles = !self.showParticles
    }
    func toggleSceneMode() {
        self.isInViewSceneMode = !self.isInViewSceneMode
    }
    func toggleRenderMode() {
        switch renderMode {
        case .particles:
            renderMode = .splats
        case .splats:
            renderMode = .both
        case .both:
            renderMode = .particles
        }
    }
    func getCpuParticles() -> Array<CPUParticle> {
        return self.cpuParticlesBuffer
    }

    /// TRUE zero-copy GPU conversion: Unpack OpenSplat buffers using GPU compute shader
    /// NO CPU eigendecomposition, NO CPU memcpy, NO per-frame allocation!
    ///
    /// Performance: Truly zero-cost after first frame
    /// - Old: 15,000 eigendecompositions √ó ~50¬µs = 750ms per frame üî•
    /// - New: GPU unpack kernel = ~0.05ms per frame ‚úÖ (1500√ó faster!)
    ///
    /// NO CPU-GPU sync! Runs in the rendering command buffer.
    private func unpackGaussiansGPU(commandBuffer: MTLCommandBuffer, count: Int) {
        guard count > 0 else { return }
        guard let unpackPipeline = unpackGaussiansPipelineState else {
            print("‚ö†Ô∏è Unpack pipeline not available")
            return
        }

        // Allocate or reuse cached buffers
        // Use .storageModeShared for debugging (allows CPU read)
        if cachedBufferCapacity < count {
            print("üì¶ Allocating unpacked buffers for \(count) gaussians (capacity: \(cachedBufferCapacity) ‚Üí \(count))")
            cachedMeans3dBuffer = device.makeBuffer(length: count * 3 * MemoryLayout<Float>.stride, options: .storageModeShared)
            cachedScalesBuffer = device.makeBuffer(length: count * 3 * MemoryLayout<Float>.stride, options: .storageModeShared)
            cachedQuatsBuffer = device.makeBuffer(length: count * 4 * MemoryLayout<Float>.stride, options: .storageModeShared)
            cachedColorsBuffer = device.makeBuffer(length: count * 3 * MemoryLayout<Float>.stride, options: .storageModeShared)
            cachedOpacitiesBuffer = device.makeBuffer(length: count * MemoryLayout<Float>.stride, options: .storageModeShared)
            cachedBufferCapacity = count
        }

        guard let means3dBuffer = cachedMeans3dBuffer,
              let scalesBuffer = cachedScalesBuffer,
              let quatsBuffer = cachedQuatsBuffer,
              let colorsBuffer = cachedColorsBuffer,
              let opacitiesBuffer = cachedOpacitiesBuffer else {
            print("‚ö†Ô∏è Failed to allocate unpacked buffers")
            return
        }

        // GPU unpack kernel - runs in the same command buffer as rendering
        // NO separate command buffer, NO waitUntilCompleted - zero CPU-GPU sync!
        guard let encoder = commandBuffer.makeComputeCommandEncoder() else { return }
        encoder.label = "Unpack OpenSplat Gaussians"
        encoder.setComputePipelineState(unpackPipeline)

        encoder.setBuffer(openSplatGaussiansBuffer.buffer, offset: 0, index: 0)
        encoder.setBuffer(means3dBuffer, offset: 0, index: 1)
        encoder.setBuffer(scalesBuffer, offset: 0, index: 2)
        encoder.setBuffer(quatsBuffer, offset: 0, index: 3)
        encoder.setBuffer(colorsBuffer, offset: 0, index: 4)
        encoder.setBuffer(opacitiesBuffer, offset: 0, index: 5)

        let threadgroupSize = MTLSize(width: 256, height: 1, depth: 1)
        let threadgroups = MTLSize(
            width: (count + threadgroupSize.width - 1) / threadgroupSize.width,
            height: 1,
            depth: 1
        )
        encoder.dispatchThreadgroups(threadgroups, threadsPerThreadgroup: threadgroupSize)
        encoder.endEncoding()

        // Update cached gaussian data
        cachedGaussianData = OpenSplatRenderer.GaussianData(
            means3d: means3dBuffer,
            scales: scalesBuffer,
            quats: quatsBuffer,
            colors: colorsBuffer,
            opacities: opacitiesBuffer,
            count: count
        )
        lastUnpackedCount = count
    }
    
    /// Render using OpenSplat tile-based renderer with GPU zero-copy unpacking
    private func renderWithOpenSplat(
        commandBuffer: MTLCommandBuffer,
        openSplatRenderer: OpenSplatRenderer,
        frame: ARFrame
    ) {
        // Use OpenSplat-native buffer (no eigendecomposition needed!)
        let gaussianCount = currentOpenSplatCount
        guard gaussianCount > 0 else { return }

        // Performance monitoring
        renderFrameCounter += 1
        print("üéØ Frame \(renderFrameCounter): Rendering \(gaussianCount) gaussians (GPU ZERO-COPY)")

        // Measure frame timing every N frames
        let currentTime = CACurrentMediaTime()
        if renderFrameCounter % frameTimingInterval == 0 {
            if lastFrameTimestamp > 0 {
                let deltaTime = currentTime - lastFrameTimestamp
                let fps = Double(frameTimingInterval) / deltaTime
                print("‚è±Ô∏è  Last \(frameTimingInterval) frames: \(String(format: "%.2f", deltaTime * 1000))ms total, \(String(format: "%.1f", fps)) FPS")
            }
            lastFrameTimestamp = currentTime
        }

        // GPU unpack: Always unpack to get latest data from accumulation
        // TODO: Optimize to only unpack when data actually changes
        print("üîÑ Unpacking \(gaussianCount) gaussians (ensuring latest data)")

        // DEBUG: Check packed buffer BEFORE unpacking
        if renderFrameCounter <= 5 {
            let src = openSplatGaussiansBuffer.values
            print("üîç DEBUG: Packed buffer BEFORE unpack (first 3 gaussians):")
            for i in 0..<min(3, gaussianCount) {
                let g = src[i]
                print("  [\(i)] pos=(\(String(format: "%.3f", g.position_x)), \(String(format: "%.3f", g.position_y)), \(String(format: "%.3f", g.position_z)))")
                print("       scale=(\(String(format: "%.4f", g.scale_x)), \(String(format: "%.4f", g.scale_y)), \(String(format: "%.4f", g.scale_z)))")
                print("       quat=(\(String(format: "%.3f", g.quat_x)), \(String(format: "%.3f", g.quat_y)), \(String(format: "%.3f", g.quat_z)), \(String(format: "%.3f", g.quat_w)))")
            }
        }

        unpackGaussiansGPU(commandBuffer: commandBuffer, count: gaussianCount)

        // IMPORTANT: Commit and wait for unpacking to complete
        // The unpacking happened in this command buffer, but openSplatRenderer.render()
        // creates its own internal buffers, so we need to finish unpacking first
        commandBuffer.commit()
        commandBuffer.waitUntilCompleted()

        guard let gaussianData = cachedGaussianData else {
            print("‚ö†Ô∏è No cached gaussian data available")
            return
        }

        // DEBUG: Check if data is valid
        if renderFrameCounter <= 2 {
            let src = openSplatGaussiansBuffer.values
            print("üîç DEBUG Frame \(renderFrameCounter): OpenSplat packed data (first 3 gaussians):")
            for i in 0..<min(3, gaussianCount) {
                let g = src[i]
                print("  [\(i)] pos=(\(String(format: "%.3f", g.position_x)), \(String(format: "%.3f", g.position_y)), \(String(format: "%.3f", g.position_z)))")
                print("       scale=(\(String(format: "%.4f", g.scale_x)), \(String(format: "%.4f", g.scale_y)), \(String(format: "%.4f", g.scale_z)))")
                print("       quat=(\(String(format: "%.3f", g.quat_x)), \(String(format: "%.3f", g.quat_y)), \(String(format: "%.3f", g.quat_z)), \(String(format: "%.3f", g.quat_w)))")
                print("       color=(\(g.color_r), \(g.color_g), \(g.color_b)) opacity=\(g.opacity)")
            }
        }

        // Extract camera parameters
        let camera = frame.camera
        let viewMatrix = camera.viewMatrix(for: orientation)

        // IMPORTANT: OpenSplat uses intrinsics-based projection, not ARKit's projection matrix!
        // Pass identity matrix - projection happens via intrinsics in the shader
        let projMatrix = matrix_identity_float4x4

        // Extract intrinsics (fx, fy, cx, cy)
        let intrinsics = camera.intrinsics
        let fx = intrinsics[0, 0]
        let fy = intrinsics[1, 1]
        let cx = intrinsics[2, 0]
        let cy = intrinsics[2, 1]
        
        // Don't apply rotateToARCamera here! It's already baked into the positions during accumulation
        // The viewMatrix alone is correct for rendering

        // DEBUG: Log camera parameters
        if renderFrameCounter <= 2 {
            print("üîç DEBUG Frame \(renderFrameCounter): Camera parameters:")
            print("  Viewport: \(Int(viewportSize.width))x\(Int(viewportSize.height))")
            print("  Intrinsics: fx=\(String(format: "%.1f", fx)), fy=\(String(format: "%.1f", fy)), cx=\(String(format: "%.1f", cx)), cy=\(String(format: "%.1f", cy))")
            print("  ViewMatrix[3] = (\(String(format: "%.3f", viewMatrix[3,0])), \(String(format: "%.3f", viewMatrix[3,1])), \(String(format: "%.3f", viewMatrix[3,2])))")

            // Camera world position
            let camPos = camera.transform.columns.3
            print("  Camera world position: (\(String(format: "%.3f", camPos.x)), \(String(format: "%.3f", camPos.y)), \(String(format: "%.3f", camPos.z)))")

            // Camera forward direction (inverse of view matrix Z column)
            let forward = SIMD3<Float>(-viewMatrix[0,2], -viewMatrix[1,2], -viewMatrix[2,2])
            print("  Camera forward: (\(String(format: "%.3f", forward.x)), \(String(format: "%.3f", forward.y)), \(String(format: "%.3f", forward.z)))")
        }

        // Call OpenSplat renderer with plain view matrix
        openSplatRenderer.render(
            gaussianData: gaussianData,
            viewMatrix: viewMatrix,
            projMatrix: projMatrix,
            intrinsics: SIMD4<Float>(fx, fy, cx, cy),
            imgWidth: Int(viewportSize.width),
            imgHeight: Int(viewportSize.height),
            outputTexture: renderDestination.currentDrawable!.texture,
            background: SIMD3<Float>(0, 0, 0)
        )
    }

    // MARK: - Viewport/Camera Setup (Phase 3)

    func saveAsPlyFile(fileName: String,
                       beforeGlobalThread: [() -> Void],
                       afterGlobalThread: [() -> Void],
                       errorCallback: (XError) -> Void,
                       format: String) {

        guard !isSavingFile else {
            return errorCallback(XError.alreadySavingFile)
        }
        guard !cpuParticlesBuffer.isEmpty else {
            return errorCallback(XError.noScanDone)
        }

        DispatchQueue.global().async {
            self.isSavingFile = true
            DispatchQueue.main.async {
                for task in beforeGlobalThread { task() }
            }

            do { self.savedCloudURLs.append(try PLYFile.write(
                    fileName: fileName,
                    cpuParticlesBuffer: &self.cpuParticlesBuffer,
                    highConfCount: self.highConfCount,
                    format: format)) } catch {
                self.savingError = XError.savingFailed
            }

            DispatchQueue.main.async {
                for task in afterGlobalThread { task() }
            }
            self.isSavingFile = false
        }
    }

    func clearParticles() {
        highConfCount = 0
        currentPointIndex = 0
        currentPointCount = 0
        cpuParticlesBuffer = [CPUParticle]()
        rgbUniformsBuffers = [MetalBuffer<RGBUniforms>]()
        pointCloudUniformsBuffers = [MetalBuffer<PointCloudUniforms>]()

        commandQueue = device.makeCommandQueue()!
        for _ in 0 ..< maxInFlightBuffers {
            rgbUniformsBuffers.append(.init(device: device, count: 1, index: 0))
            pointCloudUniformsBuffers.append(.init(device: device, count: 1, index: kPointCloudUniforms.rawValue))
        }
        particlesBuffer = .init(device: device, count: maxPoints, index: kParticleUniforms.rawValue)
    }
    
    // Request to save the next depth frame as a photo
    func saveDepthPhoto() {
        shouldSaveNextDepth = true
        print("üì∏ Depth photo will be saved on next frame")
    }

    func loadSavedClouds() {
        let docs = FileManager.default.urls(
            for: .documentDirectory, in: .userDomainMask)[0]
        savedCloudURLs = try! FileManager.default.contentsOfDirectory(
            at: docs, includingPropertiesForKeys: nil, options: .skipsHiddenFiles)
    }

    // MARK: - Data Extraction for Gaussian Splatting
    func startDataExtraction() {
        isDataExtractionEnabled = true
        frameCounter = 0
        extractedFrameCounter = 0
        createDataExtractionDirectories()
        print("Started data extraction at \(dataExtractionFPS) FPS")
    }

    func stopDataExtraction() {
        guard isDataExtractionEnabled else { return }
        isDataExtractionEnabled = false
        exportCOLMAPFiles()
        print("Stopped data extraction. Total frames: \(extractedFrameCounter)")
    }

    private func createDataExtractionDirectories() {
        let documentsURL = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask)[0]

        // Create folder with timestamp for this scan session
        let dateFormatter = DateFormatter()
        dateFormatter.dateFormat = "yyyy-MM-dd_HH-mm-ss"
        let timestamp = dateFormatter.string(from: Date())
        let scanFolderName = "scan_\(timestamp)"

        currentScanFolderName = scanFolderName

        let exportURL = documentsURL.appendingPathComponent(scanFolderName)
        let imagesURL = exportURL.appendingPathComponent("images")
        let sparseURL = exportURL.appendingPathComponent("sparse")
        let sparse0URL = sparseURL.appendingPathComponent("0")

        try? FileManager.default.createDirectory(at: exportURL, withIntermediateDirectories: true)
        try? FileManager.default.createDirectory(at: imagesURL, withIntermediateDirectories: true)
        try? FileManager.default.createDirectory(at: sparse0URL, withIntermediateDirectories: true)

        print("Created scan folder: \(scanFolderName)")
    }

    private func extractCameraData(frame: ARFrame) {
        let documentsURL = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask)[0]
        let exportURL = documentsURL.appendingPathComponent(currentScanFolderName)

        // 1. Save RGB Image
        saveRGBImage(frame: frame, to: exportURL, frameIndex: extractedFrameCounter)

        // 2. Extract and save camera intrinsics
        saveCameraIntrinsics(frame: frame, to: exportURL, frameIndex: extractedFrameCounter)

        // 3. Extract and save camera extrinsics (pose)
        saveCameraExtrinsics(frame: frame, to: exportURL, frameIndex: extractedFrameCounter)

        extractedFrameCounter += 1
    }

    private func saveRGBImage(frame: ARFrame, to baseURL: URL, frameIndex: Int) {
        // Convert ARFrame's captured image to JPEG
        let pixelBuffer = frame.capturedImage
        let ciImage = CIImage(cvPixelBuffer: pixelBuffer)
        let context = CIContext()

        // Convert to RGB color space and create JPEG data
        if let colorSpace = CGColorSpace(name: CGColorSpace.sRGB),
           let jpegData = context.jpegRepresentation(of: ciImage, colorSpace: colorSpace, options: [:]) {

            let imageURL = baseURL.appendingPathComponent("images").appendingPathComponent(String(format: "%06d.jpg", frameIndex))

            do {
                try jpegData.write(to: imageURL)
            } catch {
                print("Failed to save image \(frameIndex): \(error)")
            }
        }
    }

    private func saveCameraIntrinsics(frame: ARFrame, to baseURL: URL, frameIndex: Int) {
        let intrinsics = frame.camera.intrinsics
        let imageResolution = frame.camera.imageResolution

        // Extract intrinsic parameters
        let fx = intrinsics[0][0]  // Focal length X
        let fy = intrinsics[1][1]  // Focal length Y
        let cx = intrinsics[2][0]  // Principal point X
        let cy = intrinsics[2][1]  // Principal point Y

        // COLMAP cameras.txt format: CAMERA_ID MODEL WIDTH HEIGHT PARAMS[]
        // Using PINHOLE model: fx fy cx cy
        let cameraLine = "\(frameIndex) PINHOLE \(Int(imageResolution.width)) \(Int(imageResolution.height)) \(fx) \(fy) \(cx) \(cy)\n"

        let camerasURL = baseURL.appendingPathComponent("sparse/0/cameras.txt")

        // Append to cameras.txt file
        if let data = cameraLine.data(using: .utf8) {
            if FileManager.default.fileExists(atPath: camerasURL.path) {
                if let fileHandle = try? FileHandle(forWritingTo: camerasURL) {
                    fileHandle.seekToEndOfFile()
                    fileHandle.write(data)
                    fileHandle.closeFile()
                }
            } else {
                try? data.write(to: camerasURL)
            }
        }
    }

    private func saveCameraExtrinsics(frame: ARFrame, to baseURL: URL, frameIndex: Int) {
        // Get camera transform (camera-to-world)
        let cameraToWorld = frame.camera.transform

        // Convert to world-to-camera (what COLMAP expects)
        let worldToCamera = cameraToWorld.inverse

        // Extract rotation (as quaternion) and translation
        let rotation = simd_quaternion(worldToCamera)
        let translation = worldToCamera.columns.3

        // COLMAP images.txt format: 
        // IMAGE_ID QW QX QY QZ TX TY TZ CAMERA_ID NAME
        let imageLine = "\(frameIndex) \(rotation.real) \(rotation.imag.x) \(rotation.imag.y) \(rotation.imag.z) \(translation.x) \(translation.y) \(translation.z) \(frameIndex) \(String(format: "%06d.jpg", frameIndex))\n"

        let imagesURL = baseURL.appendingPathComponent("sparse/0/images.txt")

        // Append to images.txt file
        if let data = imageLine.data(using: .utf8) {
            if FileManager.default.fileExists(atPath: imagesURL.path) {
                if let fileHandle = try? FileHandle(forWritingTo: imagesURL) {
                    fileHandle.seekToEndOfFile()
                    fileHandle.write(data)
                    fileHandle.closeFile()
                }
            } else {
                try? data.write(to: imagesURL)
            }
        }
    }

    private func exportCOLMAPFiles() {
        let documentsURL = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask)[0]
        let exportURL = documentsURL.appendingPathComponent(currentScanFolderName)

        // Export actual point cloud data to points3D.txt
        exportPointCloudData(to: exportURL)

        print("COLMAP data exported to: \(exportURL.path)")
        print("Total extracted frames: \(extractedFrameCounter)")
        print("Total point cloud points: \(cpuParticlesBuffer.count)")
        print("High confidence points: \(highConfCount)")
        print("Files accessible in iOS Files app under: \(currentScanFolderName)")
    }
    
    private func exportPointCloudData(to baseURL: URL) {
        let pointsURL = baseURL.appendingPathComponent("sparse/0/points3D.txt")
        
        // COLMAP points3D.txt format:
        // POINT3D_ID X Y Z R G B ERROR TRACK[] as (IMAGE_ID POINT2D_IDX)
        // We'll use a simplified format with minimal track info
        
        var pointsContent = "# 3D point list with one line of data per point:\n"
        pointsContent += "# POINT3D_ID X Y Z R G B ERROR TRACK[] as (IMAGE_ID POINT2D_IDX)\n"
        
        var pointID = 0
        for particle in cpuParticlesBuffer {
            // Only export high-confidence points (confidence level 2)
            if particle.confidence >= 2 {
                let pos = particle.position
                let color = particle.color
                
                // Convert color from 0-255 float to 0-255 int
                let red = max(0, min(255, Int(color.x)))
                let green = max(0, min(255, Int(color.y)))
                let blue = max(0, min(255, Int(color.z)))
                
                // Format: POINT3D_ID X Y Z R G B ERROR TRACK[]
                // Using error = 1.0 as default, empty track for simplicity
                let pointLine = "\(pointID) \(pos.x) \(pos.y) \(pos.z) \(red) \(green) \(blue) 1.0\n"
                pointsContent += pointLine
                
                pointID += 1
            }
        }
        
        // Write to file
        if let data = pointsContent.data(using: .utf8) {
            do {
                try data.write(to: pointsURL)
                print("Exported \(pointID) high-confidence points to points3D.txt")
            } catch {
                print("Failed to write points3D.txt: \(error)")
            }
        }
    }
}

// MARK: - Metal Renderer Helpers
private extension Renderer {
    func makeUnprojectionPipelineState() -> MTLRenderPipelineState? {
        guard let vertexFunction = library.makeFunction(name: "unprojectVertex") else {
                return nil
        }

        let descriptor = MTLRenderPipelineDescriptor()
        descriptor.vertexFunction = vertexFunction
        descriptor.isRasterizationEnabled = false
        descriptor.depthAttachmentPixelFormat = renderDestination.depthStencilPixelFormat
        descriptor.colorAttachments[0].pixelFormat = renderDestination.colorPixelFormat

        return try? device.makeRenderPipelineState(descriptor: descriptor)
    }

    func makeUnprojectionOpenSplatPipelineState() -> MTLRenderPipelineState? {
        guard let vertexFunction = library.makeFunction(name: "unprojectVertexOpenSplat") else {
                return nil
        }

        let descriptor = MTLRenderPipelineDescriptor()
        descriptor.vertexFunction = vertexFunction
        descriptor.isRasterizationEnabled = false
        descriptor.depthAttachmentPixelFormat = renderDestination.depthStencilPixelFormat
        descriptor.colorAttachments[0].pixelFormat = renderDestination.colorPixelFormat

        return try? device.makeRenderPipelineState(descriptor: descriptor)
    }
    
    func makeUnpackGaussiansPipelineState() -> MTLComputePipelineState? {
        guard let kernelFunction = library.makeFunction(name: "unpackOpenSplatGaussians") else {
            print("‚ö†Ô∏è Failed to load unpackOpenSplatGaussians kernel")
            return nil
        }
        return try? device.makeComputePipelineState(function: kernelFunction)
    }

    func makeRGBPipelineState() -> MTLRenderPipelineState? {
        guard let vertexFunction = library.makeFunction(name: "rgbVertex"),
            let fragmentFunction = library.makeFunction(name: "rgbFragment") else {
                return nil
        }

        let descriptor = MTLRenderPipelineDescriptor()
        descriptor.vertexFunction = vertexFunction
        descriptor.fragmentFunction = fragmentFunction
        descriptor.depthAttachmentPixelFormat = renderDestination.depthStencilPixelFormat
        descriptor.colorAttachments[0].pixelFormat = renderDestination.colorPixelFormat

        return try? device.makeRenderPipelineState(descriptor: descriptor)
    }

    func makeParticlePipelineState() -> MTLRenderPipelineState? {
        guard let vertexFunction = library.makeFunction(name: "particleVertex"),
            let fragmentFunction = library.makeFunction(name: "particleFragment") else {
                return nil
        }

        let descriptor = MTLRenderPipelineDescriptor()
        descriptor.vertexFunction = vertexFunction
        descriptor.fragmentFunction = fragmentFunction
        descriptor.depthAttachmentPixelFormat = renderDestination.depthStencilPixelFormat
        descriptor.colorAttachments[0].pixelFormat = renderDestination.colorPixelFormat
        descriptor.colorAttachments[0].isBlendingEnabled = true
        descriptor.colorAttachments[0].sourceRGBBlendFactor = .sourceAlpha
        descriptor.colorAttachments[0].destinationRGBBlendFactor = .oneMinusSourceAlpha
        descriptor.colorAttachments[0].destinationAlphaBlendFactor = .oneMinusSourceAlpha

        return try? device.makeRenderPipelineState(descriptor: descriptor)
    }

    /// Makes sample points on camera image, also precompute the anchor point for animation
    func makeGridPoints() -> [Float2] {
        // If we have cached probability samples from PromptDA LoG, use them
        if let cached = cachedProbSamples, !cached.isEmpty {
            print("üìä Using LoG probability samples: \(cached.count) points")
            return cached
        }
        
        // Fallback to uniform hexagonal grid (original behavior for LiDAR)
        let gridArea = cameraResolution.x * cameraResolution.y
        let spacing = sqrt(gridArea / Float(numGridPoints))
        let deltaX = Int(round(cameraResolution.x / spacing))
        let deltaY = Int(round(cameraResolution.y / spacing))

        var points = [Float2]()
        for gridY in 0 ..< deltaY {
            let alternatingOffsetX = Float(gridY % 2) * spacing / 2
            for gridX in 0 ..< deltaX {
                let cameraPoint = Float2(alternatingOffsetX + (Float(gridX) + 0.5) * spacing, (Float(gridY) + 0.5) * spacing)

                points.append(cameraPoint)
            }
        }

        return points
    }

    func makeTextureCache() -> CVMetalTextureCache {
        // Create captured image texture cache
        var cache: CVMetalTextureCache!
        CVMetalTextureCacheCreate(nil, nil, device, nil, &cache)

        return cache
    }

    func makeTexture(fromPixelBuffer pixelBuffer: CVPixelBuffer, pixelFormat: MTLPixelFormat, planeIndex: Int) -> CVMetalTexture? {
        let width = CVPixelBufferGetWidthOfPlane(pixelBuffer, planeIndex)
        let height = CVPixelBufferGetHeightOfPlane(pixelBuffer, planeIndex)

        var texture: CVMetalTexture? = nil
        let status = CVMetalTextureCacheCreateTextureFromImage(nil, textureCache, pixelBuffer, nil, pixelFormat, width, height, planeIndex, &texture)

        if status != kCVReturnSuccess {
            texture = nil
        }

        return texture
    }

    static func cameraToDisplayRotation(orientation: UIInterfaceOrientation) -> Int {
        switch orientation {
        case .landscapeLeft:
            return 180
        case .portrait:
            return 90
        case .portraitUpsideDown:
            return -90
        default:
            return 0
        }
    }

    static func makeRotateToARCameraMatrix(orientation: UIInterfaceOrientation) -> matrix_float4x4 {
        // flip to ARKit Camera's coordinate
        let flipYZ = matrix_float4x4(
            [1, 0, 0, 0],
            [0, -1, 0, 0],
            [0, 0, -1, 0],
            [0, 0, 0, 1] )

        let rotationAngle = Float(cameraToDisplayRotation(orientation: orientation)) * .degreesToRadian
        return flipYZ * matrix_float4x4(simd_quaternion(rotationAngle, Float3(0, 0, 1)))
    }
}
